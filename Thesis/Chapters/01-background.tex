% !TeX root = ../main.tex
% !TeX spellcheck = en_US

\chapter{Background}
In programming languages, \emph{values} are typically associated with \emph{types} that describe the type of information the value stores\footnote{Though not all languages are typed. Especially early and low-level languages -- for example most assembly languages -- operate on untyped data.}. The \emph{type system} of a language determines what assignments are valid and which conversions between types are acceptable. A type system's rules are usually presented in the form of deduction rules, where an expression type checks if it is deduceable.

An rule might for example state that assignments are only permitted if the left- and right-hand side have the same type. As a deduction rule, the rule would look like this:

\begin{prooftree}
    \AxiomC{$\tau : \star \in \Gamma$}
    \AxiomC{$v : \tau \in \Gamma$}
    \AxiomC{$w : \tau \in \Gamma$}
    \TrinaryInfC{$\Gamma \vdash v \coloneqq w$}
\end{prooftree}

Or in normal language: The assignment $v \coloneqq w$ is valid under a context $\Gamma$ if
\begin{itemize}
    \setlength\itemsep{-0.4em}
    \item $\tau$ is a type (written $\tau : \star$) we know,
    \item $v$ is of type $\tau$ (written $v : \tau$) and
    \item $w$ is of type $\tau$.
\end{itemize}

A program is said to \emph{type check} if it follows all the rules of the type system. Type checking may occur statically (when the code is compiled) or dynamically (when the code is executed).

Strong type systems increase the reliability of software. They inform the programmer about the type of information stored in a variable and prevent the accidental mixing of values of incompatible types, such as using a string where an integer would be expected. The type system can also control the operational semantics of a program, such as Rust's ownership model to restrict memory aliasing \cite{matsakis2014rust}. Additionally, more expressive type systems allow programmers to encode their own constraints to better match the their application logic. This includes using types to represent different measurement units \cite{units-in-r} and using affine types to model use-at-most-once semantics \cite{walker2005substructural}.

Well-known type systems include the family of Hindley-Milner type systems, which form the foundation for Haskell's type system \cite{haskelldesignreport}
\todo{Expand this}

\section{The Curry-Howard Correspondence}
Mathematically, a direct connection between type systems and formal logic exists. The link was discovered by Haskell Curry and William Alvin Howard over several years and first formalized in full by Howard in 1969 \cite{howard1969formulaeastypes}.

The initial discovery made by Curry in 1934 was that the structure of proofs in propositional logic closely mirrored that of programs in typed combinatory logic (or equivalently, simply typed lambda calculus) \cite{curry1934functionality}. We get the following mapping:

\begin{table}[H]
    \begin{center}
    \begin{tabular}{ll|ll}
                    \multicolumn{2}{r|}{\textbf{Simply Typed Lambda Calculus}}  & \multicolumn{2}{l}{\textbf{Propositional Logic}} \\ \hline
        Type        & \texttt{A}            & Proposition   & $A$ \\
        Variable    & \texttt{a : A}        & Axiom         & $A$ \\
        Function    & \texttt{f : A -> B}   & Implication   & $A \to B$ \\
        Application & \texttt{f a}          & Modus Ponens  & $A \to B, A \vdash B$ \\
        Product     & \texttt{A Ã— B}        & Conjunction   & $A \land B$ \\
        Sum         & \texttt{A | B}        & Disjunction   & $A \lor B$ \\
        Unit Type   & \texttt{()}           & Tautology     & \\
        Bottom Type & \texttt{0}            & False Statement & \\
        Map into bottom type & \texttt{f : A -> 0}   & Negation       & $\lnot A$
    \end{tabular}
    \end{center}
\caption{Curry-Howard correspondence for propositional logic}
\label{tab:curryhowardprop}
\end{table}

However what happens if we now consider predicate logic instead of propositional logic? Predicate logic introduces \emph{predicates} and \emph{quantifiers} for which we need to find type theoretic analogues. \todo{cite howard, de brujin}

Predicates fill a similar role to propositions, and like propositions we are going to represent them with types. However, the truth value of predicates depends on their argument, so we cannot find a single type to represent the entire predicate. Instead, we construct a family of types indexed by the potential arguments to the predicate. In other words, if we have some predicate $p$ taking an argument of type $A$, then for each $a \oftype A$ we have a type $P(a)$ representing the truth value of the predicate for $a$. This indexed family of types is called an \emph{dependent type}.

Consider now a formula of the form $\exists x \oftype A.~p(x)$. A constructive proof of this statement consists of a some element $x \oftype A$ and a constructive proof that $p(x)$ holds. Or from the type theoretic perspective: A tuple $(x , y)$ of a witness $x \oftype A$ and an element $y \oftype P(x)$ representing the proof of $p(x)$. This is called the \emph{dependent sum type} or $\Sigma$\emph{-type} and written as $\sum_{a \oftype A}P(a)$. This notations comes from the fact that the size of the type $\sum_{a \oftype A}P(a)$ is equal to $\sum_{a \oftype A} \abs{P(a)}$. Observe that for the non-dependent special case that $P(a) = P$ for all $a$, this simplifies to $\abs A * \abs B$ and is analogous to the non-dependent product type $A \times B$.

Lastly we have formulas of the form $\forall x \oftype A.~p(x)$. A constructive proof of statements like this would require a constructive proof of $p(x)$ given any arbitrary $x$. In the type theoretic world we do this by giving a total function $(x \oftype A) \to P(x)$. This is called a \emph{dependent function}, the \emph{dependent product type} or $\Pi$\emph{-type} and written as $\Pi_{a \oftype A} P(a)$. This notation comes from the fact that for every $a \oftype A$ we can pick any proof object $b \oftype P(a)$ in our mapping and thus the total number of functions $(x \oftype A) \to P(x)$ is $\Pi_{a \oftype A} \abs{P(a)}$. Again, observe that for the non-dependent case this simplifies to ${\abs{P}}^{\abs{A}}$, the usual non-dependent function space.

\section{Agda: Dependent Types In Practice}
Agda is a functional, dependently typed language originally developed by Ulf Norell \cite{norell:thesis}. Its syntax is similar to Haskell, though significantly different in some spots, especially where dependent types are concerned. Here's how a simple datatype declarations looks like:

\begin{lstlisting}[caption={Datatype definitions},label={lst:tutorial:datatypes},emph={zero,suc}]
-- Inductive declaration of natural numbers
data bNat : Set where
    zero : bNat
    suc  : bNat -> bNat
\end{lstlisting}

Compare this to the Haskell definition of the same datatype:

\begin{lstlisting}[caption={Peano Numbers in Haskell},label={lst:haskell:peano},language=haskell,emph={Zero,Suc}]
data Nat = Zero | Suc Nat
\end{lstlisting}

Two things stand out. For one, constructors for a datatype are written in a function signature style (\texttt{suc} takes a \texttt{$\mathbb N$} and returns a new \texttt{$\mathbb N$}). Second, the datatype itself is annotated with a type (\texttt{Set}).

The latter is Agda's answer to Russel's Paradox. If a type is a set -- the collection of all members of that type -- then what is the "type" of all types? It can't be a type itself or we run into Russel's Paradox again. Instead, Agda sorts these types-of-types (and types-of-types-of-types) into a hierarchy: A value is a member of a type, a type is a member of \texttt{Set 0} -- though the 0 can be omitted --, \texttt{Set 0} is a member of \texttt{Set 1} and so on. Each of these sets is closed under the formation of (dependent) functions and pairs.

The set level of a newly declared datatype is then one larger than the maximum of all its constructors, since it needs to "contain" all values formed by use of these constructors. For example:

\begin{itemize}
    \setlength\itemsep{-0.4em}
    \item \texttt{zero : $\mathbb N$} has no arguments. Its is a value.
    \item \texttt{suc : $\mathbb N \to \mathbb N$}  has one argument, a value. Its level is zero.
    \item A hypothetical constructor \texttt{foo : (A : Set a) -> B} has one argument, a sort of level $a$. Its level is $a + 1$.
\end{itemize}

Datatypes can be \emph{parameterized} and \emph{indexed}. The two notations are largely analogous and only differ in that all constructors of a datatype must share the same parameter, but may use different indices. Parameters are automatically brought into scope for constructors, indices are not. Consider for example the following type of vectors, parameterized by the type of values it contains and indexed by its length:

\begin{lstlisting}[caption={A Simple Vector Type},label={lst:tutorial:vec},emph={Vec,append}]
data Vec (A : Set) : bNat -> Set where
    []     : Vec A zero
    append : (l : bNat) -> A -> Vec A l -> Vec A (suc l)
\end{lstlisting}

However having to explicitly pass as an argument any value that may later be used in a dependent type quickly gets cumbersome. Agda therefore has support for implicit arguments which may be omitted by the programmer when calling a function or constructor. Agda will then try to find the correct value for that parameter using unification. Rewriting our vector example:

\begin{lstlisting}[caption={A Simple Vector Type, Take Two},label={lst:tutorial:vec:2},emph={Vec,append}]
data Vec (A : Set) : bNat -> Set where
    []     : Vec A zero
    append : {l : bNat} -> A -> Vec A l -> Vec A (suc l)
\end{lstlisting}

In fact, Agda also has support to infer the \emph{type} of a parameter, though this is largely notational sugar for writing type signatures.

\begin{lstlisting}[caption={A Simple Vector Type, Take Three},label={lst:tutorial:vec:3},emph={Vec,append}]
data Vec (A : Set) : bNat -> Set where
    []     : Vec A zero
    append : forall {l} -> A -> Vec A l -> Vec A (suc l)
\end{lstlisting}

Finally, mirroring Haskell's \texttt{:} operator, we'd like an infix operator for our \texttt{append} constructor.

\begin{lstlisting}[caption={A Simple Vector Type, Take Four},label={lst:tutorial:vec:4},emph={Vec}]
data Vec (A : Set) : bNat -> Set where
    []   : Vec A zero
    _::_ : forall {l} -> A -> Vec A l -> Vec A (suc l)
\end{lstlisting}

In this case, the underscores are placeholders and are replaced by the explicit parameters of the function at call time: \texttt{\_::\_ x xs} becomes \texttt{x :: xs}. Agda not only supports this for infix operators, but also postfix and and even mixfix operators. This means that for example \texttt{if\_then\_else\_ : $\forall$ {A} -> Bool -> A -> A -> A} is a valid function.

The usual functional aspects like pattern matching and higher order functions work as expected:

\begin{lstlisting}[caption={Functions},label={lst:tutorial:functional},emph={foldr,sum}]
_+_ : bNat -> bNat -> bNat
zero + n    = n
(suc m) + n = suc (m + n)

foldr : forall {A B} -> forall {l} -> (A -> B -> B) -> B -> Vec A l -> B
foldr _ initial []        = initial
foldr f initial (x :: xs) = f x (foldr xs f initial)

sum : forall {l} -> Vec bNat l -> bNat
sum = foldr (_+_) 0
\end{lstlisting}

\subsection{Termination Checking}
Agda blurs the line between compile time and run time. Since any value may be used as a dependent parameter, the type checker may need to evaluate arbitrary chunks of code at compile time. To ensure that type checking remains decidable, Agda enforces strict termination of recursive functions.

By default Agda checks for termination by way of structural recursion. As a first intuition, we can consider a recursive call to be permitted if it only uses a strict sub-expression of the argument in the call. If there's more than one argument to the function, recursion is submitted if strict lexicographical ordering on the size of the arguments can be found (i.e. one argument either strictly decreases or it remains equal but the remainder of the arguments is strictly decreasing).

This recursion structure is often applicable, but there are times where it is insufficient. Consider for example the following implementation of a base 2 logarithm (setting $\log_2 0 \coloneqq 0$ for simplicity's sake):

\begin{lstlisting}[caption={Base 2 Logarithm},label={lst:tutorial:log},emph={log}]
\lfloorlog\_2_\rfloor : bNat -> bNat
\lfloorlog\_2 0 \rfloor = 0
\lfloorlog\_2 1 \rfloor = 0
\lfloorlog\_2 n@(suc (suc _)) = \lfloorlog\_2 \lfloor n /2\rfloor \rfloor
\end{lstlisting}

Even though this function obviously terminates, \texttt{$\lfloor$ n /2$\rfloor$} is too complex to analyze and Agda is not able to show termination.

The type-theoretic answer to this is well-founded recursion. This means that we can define an ordering on our parameters so that there is no infinite descending chain of parameters in our recursive calls. It follows that any call to our function necessarily terminates. However, in constructive mathematics a negated existential such as "no infinite descending chain" is not a very useful statement -- all it allows us is to derive a contradiction if we are given an infinite descending chain \cite{mietek:2020:minimal}. A more constructive way to talk about this concept is the notion of \emph{accessibility}. Elements are defined as accessible inductively according to the following rule:

\begin{prooftree}
    \AxiomC{$\forall y < x~:~y~\mathrm{accessible}$}
    \UnaryInfC{$x~\mathrm{accessible}$}
\end{prooftree}

We call $Acc(A, <)$ the accessible core of $A$. If $A = Acc(A, <)$, i.e. all of $A$ is accessible, we can show that $<$ is well-founded:

\begin{lemma}
    Let $x \in A$ be accessible. Then all descending chains starting with $x$ are finite.
\end{lemma}

\begin{proof}
    We show this by transfinite induction on the depth of the deduction tree showing that $x$ is accessible, $d$.

    \emph{$d = 0$}: Then $x$ must not have any predecessors. There can be no non-trivial chain starting at $x$.

    \emph{$d = d' + 1$ for some successor ordinal $d'$ or $d' = 0$}: Then every $y < x$ must have a proof depth less than $d$. By the inductive hypothesis, all chains starting at $y$ are finite and therefore all chains $x > y > \ldots$ are finite as well.

    \emph{$d = \omega + 1$ for some limit ordinal $\omega$}: No proof tree can have a depth of exactly $\omega$ \todo{show this}. Thus every $y < x$ must have a depth $\lambda_y + n_y$ for some limit ordinal $\lambda_y < \omega$ (possibly 0). Then by the inductive hypothesis all chains starting at $y$ are finite and therefore all chains $x > y > \ldots$ are finite as well.
\end{proof}

The accessibility relationship is formalized in Agda as follows:

\begin{lstlisting}[caption={The Acc Type},label={lst:tutorial:acc},emph={Acc,acc}]
data Acc (_<_ : Rel A l) (x : A) : Set (a \lub l) where
    acc : (\forall y -> y < x -> Acc _<_ y) -> Acc _<_ x
\end{lstlisting}

In normal words, we can show an element \texttt{x} accessible under some relation \texttt{\_<\_} if we can provide a function that shows any predecessor of \texttt{x} to be accessible.

We can leverage this type to show that our log function terminates:

\begin{lstlisting}[caption={Base 2 Logarithm},label={lst:tutorial:log:2},emph={log,step},add to literate={s\\leqs}{{s\(\leq\)s}}3{z\\leqn}{{z\(\leq\)n}}3{<=>\\leq}{{<\(\Rightarrow\leq\)}}3]
n>0=>\lfloorn/2\rfloor<n : (n : bNat) -> \lfloor suc n /2\rfloor < suc n
n>0=>\lfloorn/2\rfloor<n 0 = s\leqs z\leqn
n>0=>\lfloorn/2\rfloor<n 1 = s\leqs (s\leqs z\leqn)
n>0=>\lfloorn/2\rfloor<n (suc (suc n)) = s\leqs (s\leqs (<=>\leq (n>0=>\lfloorn/2\rfloor<n n)))

\lfloorlog\_2_\rfloor : bNat -> bNat
\lfloorlog\_2 n \rfloor = log\_2-step n (<-wellFounded n)
  where
    log\_2-step (n : bNat) -> Acc _<_ n -> bNat
    log\_2-step 0 _ = 0
    log\_2-step 1 _ = 0
    log\_2-step n@(suc n-1@(suc _)) (acc wf) = log\_2-step \lfloor n /2\rfloor
        (wf n (n>0=>\lfloorn/2\rfloor<n n-1))
\end{lstlisting}

In line 1-4, we are showing that $\lfloor \frac n 2 \rfloor$ is strictly smaller than $n$ for $n > 0$. This is a prerequisite for our termination proof.

In line 9 we parameterize our log function by an additional parameter that formalizes the fact that we are using the well-foundedness of the natural numbers under $<$ in general and the accessibility of our parameter \texttt{n} in general to show termination.

In line 13 we are using the fact that $\lfloor \frac n 2 \rfloor < n$ must be accessible as well to generate the accessibility proof of it and pass that to the recursive call.

Why does the termination checker permit this recursion scheme? After all, \texttt{wf n \_} is not a sub-expression of \texttt{acc wf}. However, Agda does not only consider \texttt{x} $<$ \texttt{C x} for all constructors \texttt{C}, but also \texttt{f x} $\leq$ \texttt{f} for all functions \texttt{f}. One can show this ordering on terms to be well-founded as well \cite{abel:2002:predicative-analysis-structural-recursion}, which gives justification to the termination checker.


\subsection{Equality}
\todo{Intro}

\begin{lstlisting}[caption={Equality in Agda},label={lst:tutorial:equality},emph={refl}]
data _\equiv_ {a} {A : Set a} (x : A) : A -> Set where
    refl : x \equiv x
\end{lstlisting}

In other words, we can only show the trivial equality. This may seem restrictive, but more complicated proofs of equality can be built from this basic block: transitivity, symmetry and congruence all become trivial the moment we pattern match on the constructor of \texttt{\_$\equiv$\_} and Agda realizes that the two sides of the equality are definitionally equal:

\begin{lstlisting}[caption={Reasoning in Agda},label={lst:tutorial:reasoning
},emph={sym,cong,trans,subst}]
sym : {x y : A} -> x \equiv y -> y \equiv x
sym refl = refl

trans : {x y z : A} -> x \equiv y -> y \equiv z -> x \equiv z
trans refl refl = refl

cong : {x y : A} -> (f : A -> B) -> x \equiv y -> f x \equiv f y
cong f refl = refl

subst : {x y : A} -> (P : A -> Set a) -> x \equiv y -> P x -> P y
subst P refl p = p
\end{lstlisting}

\todo{Concluding remarks?}