% !TeX root = ../main.tex
% !TeX spellcheck = en_US

\chapter{Amortized Analysis}
After having talked about deterministic worst-case bounds, let us now turn towards amortized analysis. Amortized analyses consider aggregate sequences of operations (usually starting from some minimum element). This accounts for data structures where operations are usually fast, but occasionally slow. An example that may be familiar to a lot of people is C++'s \texttt{std::vector} and Java's \texttt{ArrayList}: Enough space for $n$ elements is allocated on the heap. This allows for $n$ insertions in constant time. On the $(n+1)$-th operation, twice as much space is allocated and the previous $n$ elements are copied over. This take $O(n)$ time, which if offset against the previous operations results in \emph{amortized} constant time.

There are two main approaches to an amortized analysis: the banker's method and the potential method. They differ largely in how the calculate the cost for an individual operation. We will use the potential method in this thesis.

\section{The Potential Method}
Intuitively, the potential method assigns to a data structure a measure of ``potential energy". An operation should generally increase this potential method when it takes little time, but prepares the stage for a long operation. Conversely, an operation that takes a long time should use this accumulated potential energy to offset its cost. In the example from above, an insert operation would increase the potential by one, while reallocating the list would reduce the potential by the number of elements in the list.

Let $S$ be the type of a data structure and $s \in S$ any instance of this data structure. The potential method assigns a potential to every such $s$: $\varphi : S -> D$, where $D$ is some numerical domain. In this thesis, we will use $\mathbb N$ but $\mathbb Q$ or $mathbb R$ are equally possible. We ask two things of $\varphi$:

\begin{itemize}
    \item The potential is always positive, $\varphi(s) \geq 0$.
    \item There is an ``initial" object $s_0$ from which the analysis starts (e.g. an empty list or an empty tree) and its potential is always 0: $\varphi(s_0) = 0$.
\end{itemize}

Let $o_i$ be an operation that takes an object $s_i$ to an object $s_{i+1}$ and let $t(o_i)$ be the time it takes to compute $o_i$. Then we define the amortized time $t_{am}(o_i) = T(o_i) - \varphi(s_i) + \varphi(s_{i+1})$. For a sequence of operations $O = o_0, \ldots, o_n$ the total actual runtime is $T(O) = \sum_{i = 0}^n t(o_i)$ and the total amortized runtime is $T_{am}(O) = \sum_{i = 0}^n t_{am}(o_i) = \sum_{i = 0}^n t(o_i) - \varphi(s_i) + \varphi(s_{i+1})$. Since this is a telescoping sum, we can simplify it to $T_{am}(O) = \left(\sum_{i = 0}^n t(o_i)\right) - \varphi(s_0) + \varphi(s_{n+1})$. Since $\varphi(s_0) = 0$, this further reduces to $T_{am}(O) = T(O) + \varphi(s_{n+1})$ -- the amortized runtime is therefore an upper bound on the actual runtime.

\section{Ephemeral and Persistent Models of Computation}
In addition to the banker's and the potential method, amortized analysis also distinguishes two models of computation: The ephemeral and the persistent model. In the ephemeral model, objects can be used at most once. If they are used as input to an operation, they are replaced by the result. In the persistent model, previous states of the data structure remain accessible and can be reused as input to operations.

Going back to our vector example, in an ephemeral computing model, a list that has no remaining capacity can be inserted into only once. In a persistent model, the same list could be copied several times in this state, resulting in multiple high-cost operations. Clearly our analysis from above does not hold up in this case. For simplicity's sake, we will restrict ourselves to the ephemeral model of computation.

\section{Data Structures for an Amortized Analysis}
We introduce two datatypes: A record that defines the potential for a given type and a type to construct an amortized computation.

\begin{lstlisting}[caption={Record defining the potential for a type A},label={lst:amortized:framework:potential},emph={Amortized,initial,potential,init}]
record Amortized (A : I -> Set a) : Set (a l\lub i) where
    field
        {i\_0} : I
        initial : A i\_0
        potential : {i : I} -> A i -> bNat
        init\equiv0 : potential initial \equiv 0
\end{lstlisting}

The potential defining record contains three fields of note: The definition of the potential function itself, the initial element of the data type and a proof that the potential of the initial object is in fact zero.

The datatype \texttt{Am} on the other hand provides us with several building blocks to construct an amortized computation.

\begin{lstlisting}[caption={Building blocks of an amortized computation},label={lst:amortized:framework:computation},emph={Am,Amortized,base,init,comp,step,bind,map,initial,DecTree}]
data Am (am : Amortized A) (C : Set b) : Set (lsuc (a l\lub i l\lub b)) where
    base :  (x : A (Amortized.i\_0 am))
         -> {x \equiv Amortized.initial am}
         -> Am am C
    init-comp :  {B : Set a}
                 {time : bNat}
              -> DecTree C B time
              -> (B -> Am am C)
              -> Am am C
    step :  {next : A i -> I}
            {time : A i -> bNat}
         -> Am am C
         -> ((x : A i) -> DecTree C (A $ next x) (time x))
         -> Am am C
    am-bind :  {J : Set i}
               {B : J -> Set a}
               {am' : Amortized B}
            -> Am am' C
            -> (B j -> Am am C)
            -> Am am C
    am-map :  {J : Set i}
              {B : J -> Set a}
              {am' : Amortized B}
              {imap : B j -> I}
           -> (f : (x : B j) -> A $ imap x)
           -> {map-is-pot-invariant :
                   (x : B j)
                   ->   Amortized.potential am' x
                      \equiv Amortized.potential am (f x)}
           -> Am am' C
           -> Am am  C
\end{lstlisting}

The simplest here is \texttt{base}, which just contains the initial element and thus forms the base of the amortized computation. It is followed by \texttt{step} which allows us to compose computations: Given an amortized computation, and a deterministic next step, this results again in an amortized computation.

Three additional constructors allow us to be more flexible in our handling of amortized computations. First, \texttt{init-comp} allows us to follow a deterministic computation with an amortized one. Second, \texttt{am-bind} allows us to take an amortized computation in one type and chain it into an amortized computation of a second type. Lastly, \texttt{am-map} allows us to transform an amortized computation of one type entirely into one of another type, as long as we can prove that the potential for both is invariant under the map.

\subsection{A Note on Indexed Types}
An observant reader might notice that we use specifically indexed types. Why is that?

Consider the case of Agda's \texttt{Vec (A : Set) : $\mathbb N$ -> Set}. This is a \emph{family} of indexed types and \texttt{Vec A 0} and \texttt{Vec A 1} are distinct types. That means if we used simple types in our definition of \texttt{Amortized}, we'd need separate declarations of it for all vectors of different lengths -- including distinct minimum elements.

Instead we allow for a family of indexed type with one parameter. For example, this allows us to generalize over vectors of arbitrary length for some fixed type, but not over vectors of arbitrary lengths and arbitrary types. Finding a way to abstract the number of parameters to our family of indexed types would have been ideal, but we were not able to find such a method within the time constraints of this thesis (nipkow \todo{ref} had a promising approach that we were unable to evaluate). Instead, as a proof of concept for this thesis, we special cased our framework to indexed families with one and with two parameters.

\section{Proof of Correctness}
We can now define a measure of the amortized runtime and show that it is an upper bound on the deterministic runtime. We first define an evaluation function for an amortized computation:

\begin{lstlisting}[caption={Evaluating amortized computations},label={lst:amortized:framework:eval},emph={am,index,eval}]
am-index : Am am Compare -> I
am-eval : {{_ : Leq C}} -> (v : Am am Compare) -> A (am-index v)

am-eval (base v) = v
am-eval (step val trans) = reduce $ trans $ am-eval val
am-eval (am-map f val) = f (am-eval val)
am-eval (am-bind val trans) = am-eval $ trans $ am-eval val
am-eval (init-comp comp trans) = am-eval $ trans $ reduce comp
\end{lstlisting}

The potential of a computation is then simply the potential of the result of the computation:

\begin{lstlisting}[caption={Potential of a computation},label={lst:amortized:framework:pot},emph={am,potential}]
am-potential : Am am Compare -> bNat
am-potential {am = am} v = Amortized.potential am $ am-eval v
\end{lstlisting}

The actual time of a computation is the sum of the worst-case guarantees of all individual steps:

\begin{lstlisting}[caption={Actual Time},label={lst:amortized:framework:actualtime},emph={dtime,full}]
dtime-full : Am am Compare -> bNat
dtime-full (base val) = 0
dtime-full (step {_} {time} inner _) =
                    dtime-full inner
                    + time $ am-eval val
dtime-full (am-map _ val) = dtime-full val
dtime-full (am-bind val trans) =
                    dtime-full val
                    + dtime-full (trans $ am-eval val)
dtime-full (init-comp {_} {time} comp trans) =
                    time
                    + dtime-full (trans $ reduce comp)
\end{lstlisting}

The amortized time costs are defined as above, actual time less previous potential plus new potential for a computational step and just the deterministic time for an initial computation:
\begin{lstlisting}[caption={Amortized Time},label={lst:amortized:framework:amortizedtime},emph={dtime,atime,step,full}]
atime-step : Am am Compare -> \bZ
atime-step (base val) = 0\bZ
atime-step (am-map _ _) = 0\bZ
atime-step (am-bind _ _) = 0\bZ
atime-step (init-comp {_} {time} _ _) = + time
atime-step v@(step val transform) = dtime-step v
                                    \ominus   pot-before
                                    \bZ.+ (+ pot-after)
    where
        val-before = am-eval val
        val-after = reduce $ transform $ val-before
        pot-before = Amortized.potential am $ val-before
        pot-after = Amortized.potential am val-after

atime-full : Am am Compare -> \bZ
atime-full (base val) = 0\bZ
atime-full (am-map _ val) = atime-full val
atime-full v@(step val _) =
                        atime-full val
                        \bZ.+ atime-step v
atime-full v@(init-comp comp trans) =
                        atime-step v
                        \bZ.+ atime-full (trans $ reduce comp)
atime-full (am-bind val trans) =
                        atime-full val
                        \bZ.+ atime-full (trans $ am-eval val)
\end{lstlisting}

All of this allows us to finally prove the central theorem of amortized runtime, that $T_{am}(O) \geq T(O) + \varphi(s_{n+1})$. The body of the proof is too long to print here and we refer the interested reader to the repository mentioned in \todo{ref to final lay of land chapter}

\begin{lstlisting}[caption={Theorem: Amortized Time is an upper bound on Actual Time},emph={atime,dtime,pot,full,potential}]
atime\geqdtime :  (v : Am am Compare)
               -> atime-full v \bZ.\geq + (dtime-full v + am-potential v)
\end{lstlisting}